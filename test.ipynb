{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Waste Classification Model"
      ],
      "metadata": {
        "id": "r6DUt7Nu_4jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMnlFlc34v4P",
        "outputId": "abd06bca-fd5c-472d-da58-2f34b2902cd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch import (nn, optim)\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import (transforms, datasets, models)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "plt.rcParams[\"font.family\"] = \"monospace\""
      ],
      "metadata": {
        "id": "6x3PwNWW5ECs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "TCFTXhh-_2Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DTYPE = torch.float32\n",
        "# Path for 'data'\n",
        "PATH = '/content/drive/MyDrive/SAMSUNGBootcampFiles/data'   # Change for different user\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-3"
      ],
      "metadata": {
        "id": "2lafKiGW5Xpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "9HF5Hq-L_-7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = datasets.ImageFolder(\n",
        "    root=PATH,\n",
        "    transform=transforms.ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "Ysc8T8oG6Mnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.class_to_idx   # Classes as a dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaVob7m59_ng",
        "outputId": "4b2ba2a3-5b69-4852-9d37-02a8892d15c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, val_set = torch.utils.data.random_split(data, [.9, .1], generator=torch.Generator().manual_seed(42))\n",
        "print(f\"Train Set: {len(train_set)}\")\n",
        "print(f\"Validation Set: {len(val_set)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCJgCox77RE4",
        "outputId": "e6a49498-5e10-4c88-c6a6-43bd7e36afd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set: 2252\n",
            "Validation Set: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH6_D3Ik7bgZ",
        "outputId": "f5491fa7-7dfa-4547-99c1-d104eb122e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Creation `mobilenetv3`"
      ],
      "metadata": {
        "id": "D717RE3vABOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = models.mobilenet_v3_small()\n",
        "Model.classifier[3] = nn.Linear(Model.classifier[3].in_features, 6)  # Adjust for 6 classes\n",
        "\n",
        "Model = Model.to(DEVICE)"
      ],
      "metadata": {
        "id": "gy4yqZhf8Kti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(Model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "bZkyNYaS8o9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and Optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = correct / total\n",
        "    print(f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "id": "zrcHIh5f_NK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Statistics\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = val_loss / len(val_loader)\n",
        "    epoch_accuracy = correct / total\n",
        "    print(f\"Validation Loss: {epoch_loss:.4f}, Validation Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "id": "Jr5Z8uR9_SNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    train_loss, train_acc = train(Model, train_loader, optimizer, criterion, DEVICE)\n",
        "    val_loss, val_acc = evaluate(Model, val_loader, criterion, DEVICE)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO2JccHC_U_B",
        "outputId": "07ad7d03-c729-4c8e-ff47-23019d3b3453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 1.3399, Train Accuracy: 0.4698\n",
            "Validation Loss: 1.7540, Validation Accuracy: 0.2080\n",
            "Epoch 2/20\n",
            "Train Loss: 1.0791, Train Accuracy: 0.5955\n",
            "Validation Loss: 1.8104, Validation Accuracy: 0.2000\n",
            "Epoch 3/20\n",
            "Train Loss: 0.9490, Train Accuracy: 0.6479\n",
            "Validation Loss: 1.8075, Validation Accuracy: 0.2000\n",
            "Epoch 4/20\n",
            "Train Loss: 0.8196, Train Accuracy: 0.7065\n",
            "Validation Loss: 2.2980, Validation Accuracy: 0.2080\n",
            "Epoch 5/20\n",
            "Train Loss: 0.7602, Train Accuracy: 0.7242\n",
            "Validation Loss: 2.8603, Validation Accuracy: 0.2080\n",
            "Epoch 6/20\n",
            "Train Loss: 0.6641, Train Accuracy: 0.7496\n",
            "Validation Loss: 3.1561, Validation Accuracy: 0.2880\n",
            "Epoch 7/20\n",
            "Train Loss: 0.6016, Train Accuracy: 0.7882\n",
            "Validation Loss: 2.9842, Validation Accuracy: 0.2720\n",
            "Epoch 8/20\n",
            "Train Loss: 0.5517, Train Accuracy: 0.7984\n",
            "Validation Loss: 2.8243, Validation Accuracy: 0.3080\n",
            "Epoch 9/20\n",
            "Train Loss: 0.4874, Train Accuracy: 0.8295\n",
            "Validation Loss: 1.6595, Validation Accuracy: 0.4880\n",
            "Epoch 10/20\n",
            "Train Loss: 0.4564, Train Accuracy: 0.8304\n",
            "Validation Loss: 3.3605, Validation Accuracy: 0.3680\n",
            "Epoch 11/20\n",
            "Train Loss: 0.3745, Train Accuracy: 0.8699\n",
            "Validation Loss: 2.0520, Validation Accuracy: 0.4960\n",
            "Epoch 12/20\n",
            "Train Loss: 0.3675, Train Accuracy: 0.8734\n",
            "Validation Loss: 1.7319, Validation Accuracy: 0.6160\n",
            "Epoch 13/20\n",
            "Train Loss: 0.2973, Train Accuracy: 0.8974\n",
            "Validation Loss: 2.4923, Validation Accuracy: 0.5040\n",
            "Epoch 14/20\n",
            "Train Loss: 0.2697, Train Accuracy: 0.9090\n",
            "Validation Loss: 2.6770, Validation Accuracy: 0.4920\n",
            "Epoch 15/20\n",
            "Train Loss: 0.2557, Train Accuracy: 0.9090\n",
            "Validation Loss: 2.9948, Validation Accuracy: 0.4760\n",
            "Epoch 16/20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Training Metrics\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.title(\"Loss over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MPM1EMmS_cSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Validation Metrics\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.title(\"Accuracy over Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-1H81FfR_gQP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}